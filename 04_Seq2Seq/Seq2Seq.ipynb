{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:11.478626Z",
     "start_time": "2024-11-23T14:11:11.459508Z"
    }
   },
   "source": [
    "# 构建语料库，每行包括中文、英文（解码器输入）和翻译成英文后的目标输出3共和句子\n",
    "sentences = [\n",
    "    ['咖哥 喜欢 小冰', '<sos> KaGe likes XiaoBing', 'KaGe likes XiaoBing <eos>'],\n",
    "    ['我 爱 学习 人工智能', '<sos> I love studying AI', 'I love studying AI <eos>'],\n",
    "    ['深度学习 改变 世界', '<sos> DL changed the world', 'DL changed the world <eos>'],\n",
    "    ['自然 语言 处理 很 强大', '<sos> NLP is so powerful', 'NLP is so powerful <eos>'],\n",
    "    ['神经网络 非常 复杂', '<sos> Neural-Nets are complex', 'Neural-Nets are complex <eos>'],\n",
    "]\n",
    "word_list_cn, word_list_en = [], []  # 中英文词汇表\n",
    "# 遍历每一个句子并将单词添加到对应的词汇表中\n",
    "for cn, en, en2 in sentences:\n",
    "    word_list_cn.extend(cn.split())\n",
    "    word_list_en.extend(en.split())\n",
    "    word_list_en.extend(en2.split())\n",
    "# 去重\n",
    "word_list_cn = list(set(word_list_cn))\n",
    "word_list_en = list(set(word_list_en))\n",
    "# 构建单词到索引的映射\n",
    "word2idx_cn = {w: i for i, w in enumerate(word_list_cn)}\n",
    "word2idx_en = {w: i for i, w in enumerate(word_list_en)}\n",
    "# 构建索引到单词的映射\n",
    "idx2word_cn = {i: w for i, w in enumerate(word_list_cn)}\n",
    "idx2word_en = {i: w for i, w in enumerate(word_list_en)}\n",
    "# 计算词汇表的大小\n",
    "vocab_size_cn = len(word_list_cn)\n",
    "vocab_size_en = len(word_list_en)\n",
    "print(\"句子数量: \", len(sentences))\n",
    "print(\"中文词汇表大小: \", vocab_size_cn)\n",
    "print(\"英文词汇表大小: \", vocab_size_en)\n",
    "print(\"中文词汇表: \", word2idx_cn)\n",
    "print(\"英文词汇表: \", word2idx_en)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量:  5\n",
      "中文词汇表大小:  18\n",
      "英文词汇表大小:  20\n",
      "中文词汇表:  {'爱': 0, '非常': 1, '语言': 2, '深度学习': 3, '强大': 4, '很': 5, '喜欢': 6, '我': 7, '神经网络': 8, '改变': 9, '复杂': 10, '世界': 11, '处理': 12, '小冰': 13, '人工智能': 14, '自然': 15, '学习': 16, '咖哥': 17}\n",
      "英文词汇表:  {'Neural-Nets': 0, 'love': 1, 'I': 2, 'world': 3, 'AI': 4, 'complex': 5, 'is': 6, '<eos>': 7, 'likes': 8, 'DL': 9, 'NLP': 10, 'the': 11, 'XiaoBing': 12, 'powerful': 13, '<sos>': 14, 'changed': 15, 'KaGe': 16, 'studying': 17, 'so': 18, 'are': 19}\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:11.509723Z",
     "start_time": "2024-11-23T14:11:11.498206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "# 定义一个函数，随机选择一个句子和词汇表表示生成输入，输出和目标数据\n",
    "def make_data(sentences):\n",
    "    # 随机选择一个句子进行训练\n",
    "    sentence = random.choice(sentences)\n",
    "    # 将输入句子中的单词转化为对应的索引\n",
    "    encoder_input = np.array([[word2idx_cn[w] for w in sentence[0].split()]])\n",
    "    # 将输出句子中的单词转化为对应的索引\n",
    "    decoder_input = np.array([[word2idx_en[w] for w in sentence[1].split()]])\n",
    "    # 将目标句子中的单词转化为对应的索引\n",
    "    target = np.array([[word2idx_en[w] for w in sentence[2].split()]])\n",
    "    # 将输入、输出、目标批次转化为LongTensor\n",
    "    encoder_input = torch.LongTensor(encoder_input)\n",
    "    decoder_input = torch.LongTensor(decoder_input)\n",
    "    target = torch.LongTensor(target)\n",
    "    return encoder_input, decoder_input, target\n",
    "# 使用make_data函数生成输入、输出和目标张量\n",
    "encoder_input, decoder_input, target = make_data(sentences)\n",
    "original_sentence = None\n",
    "for s in sentences:  # 获取原始句子\n",
    "    if all([word2idx_cn[w] in encoder_input[0] for w in s[0].split()]):\n",
    "        original_sentence = s\n",
    "        break\n",
    "print(\"原始句子: \", original_sentence)\n",
    "print(\"编码器输入张量的形状：\", encoder_input.shape)\n",
    "print(\"解码器输入张量的形状：\", decoder_input.shape)\n",
    "print(\"目标张量的形状：\", target.shape)\n",
    "print(\"编码器输入张量：\", encoder_input)\n",
    "print(\"解码器输入张量：\", decoder_input)\n",
    "print(\"目标张量：\", target)"
   ],
   "id": "64fc76da9ee79e09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始句子:  ['我 爱 学习 人工智能', '<sos> I love studying AI', 'I love studying AI <eos>']\n",
      "编码器输入张量的形状： torch.Size([1, 4])\n",
      "解码器输入张量的形状： torch.Size([1, 5])\n",
      "目标张量的形状： torch.Size([1, 5])\n",
      "编码器输入张量： tensor([[ 7,  0, 16, 14]])\n",
      "解码器输入张量： tensor([[14,  2,  1, 17,  4]])\n",
      "目标张量： tensor([[ 2,  1, 17,  4,  7]])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:11.540971Z",
     "start_time": "2024-11-23T14:11:11.527263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "# 定义编码器类和解码器类\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐藏层大小\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # 词嵌入层\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)  # RNN层\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # 词嵌入\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "# 定义解码器类\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐藏层大小\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)  # 词嵌入层\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)  # RNN层\n",
    "        self.out = nn.Linear(hidden_size, output_size)  # 输出层\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)  # 词嵌入\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.out(output)  # 输出\n",
    "        return output, hidden\n",
    "n_hidden = 128  # 隐藏层大小\n",
    "# 创建编码器和解码器\n",
    "encoder = Encoder(vocab_size_cn, n_hidden)\n",
    "decoder = Decoder(n_hidden, vocab_size_en)\n",
    "print(\"编码器结构：\", encoder)\n",
    "print(\"解码器结构：\", decoder)"
   ],
   "id": "6b61bd88135a0829",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码器结构： Encoder(\n",
      "  (embedding): Embedding(18, 128)\n",
      "  (rnn): RNN(128, 128, batch_first=True)\n",
      ")\n",
      "解码器结构： Decoder(\n",
      "  (embedding): Embedding(20, 128)\n",
      "  (rnn): RNN(128, 128, batch_first=True)\n",
      "  (out): Linear(in_features=128, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:11.587795Z",
     "start_time": "2024-11-23T14:11:11.573072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 组合编码器和解码器\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder  # 编码器\n",
    "        self.decoder = decoder  # 解码器\n",
    "    def forward(self, encoder_input, hidden, decoder_input):\n",
    "        # 使输入序列通过编码器冰获取输出和隐藏状态\n",
    "        encoder_output, encoder_hidden = self.encoder(encoder_input, None)\n",
    "        # 将编码器的隐藏状态作为解码器的初始隐藏状态\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # 使解码器的输入序列通过解码器并获取输出\n",
    "        decoder_output, _ = self.decoder(decoder_input, decoder_hidden)\n",
    "        return decoder_output\n",
    "# 创建Seq2Seq模型\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "print(\"Seq2Seq模型结构：\", model)"
   ],
   "id": "73ad6c4e3e7b8a47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq模型结构： Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(18, 128)\n",
      "    (rnn): RNN(128, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(20, 128)\n",
      "    (rnn): RNN(128, 128, batch_first=True)\n",
      "    (out): Linear(in_features=128, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:13.168096Z",
     "start_time": "2024-11-23T14:11:11.615401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义训练函数\n",
    "def train_seq2seq(model, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        encoder_input, decoder_input, target = make_data(sentences)  # 生成输入、输出和目标数据\n",
    "        hidden = torch.zeros(1, encoder_input.size(0), n_hidden)  # 初始化隐藏状态\n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, hidden, decoder_input)\n",
    "        loss = criterion(output.view(-1, vocab_size_en), target.view(-1))\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\"Epoch: %d, Loss: %1.04f\" % (epoch + 1, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# 训练模型\n",
    "epochs = 400\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_seq2seq(model, criterion, optimizer, epochs)"
   ],
   "id": "5b979825d827c694",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.0740\n",
      "Epoch: 200, Loss: 0.0184\n",
      "Epoch: 300, Loss: 0.0096\n",
      "Epoch: 400, Loss: 0.0064\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:22:51.837677Z",
     "start_time": "2024-11-23T14:22:51.814367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义预测函数\n",
    "def test_seq2seq(model, source_sentences):\n",
    "    # 将输入句子转化为索引\n",
    "    encoder_input = np.array([[word2idx_cn[w] for w in source_sentences.split()]])\n",
    "    # 构建输出句子的索引，以'<sos>'开始，后面跟'<eos>'，长度与输入句子相同\n",
    "    decoder_input = np.array([[word2idx_en['<sos>']] + [word2idx_en['<eos>']] * (len(encoder_input[0]) - 1)])\n",
    "    # 将输入、输出转化为LongTensor\n",
    "    encoder_input = torch.LongTensor(encoder_input)\n",
    "    decoder_input = torch.LongTensor(decoder_input)\n",
    "    hidden = torch.zeros(1, encoder_input.size(0), n_hidden)  # 初始化隐藏状态\n",
    "    predicted = model(encoder_input, hidden, decoder_input)  # 预测\n",
    "    predicted = predicted.data.max(2, keepdim=True)[1]  # 获取预测结果\n",
    "    # 打印输入结果何预测结果\n",
    "    print(source_sentences, '->', ' '.join([idx2word_en[i.item()] for i in predicted.squeeze()]))\n",
    "# 测试模型\n",
    "test_seq2seq(model, '咖哥 喜欢 小冰')\n",
    "test_seq2seq(model, \"自然 语言 处理 很 强大\")\n",
    "test_seq2seq(model, '我 爱 学习 人工智能')\n",
    "test_seq2seq(model, '小冰 喜欢 咖哥')"
   ],
   "id": "d3149f04ed2dbe89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咖哥 喜欢 小冰 -> KaGe likes XiaoBing\n",
      "自然 语言 处理 很 强大 -> NLP is so powerful <eos>\n",
      "我 爱 学习 人工智能 -> I love studying <eos>\n",
      "小冰 喜欢 咖哥 -> KaGe likes XiaoBing\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:11:13.293317Z",
     "start_time": "2024-11-23T14:11:13.285323Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "babd711b88df9be7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
