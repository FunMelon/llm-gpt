{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T16:37:17.093565Z",
     "start_time": "2024-11-28T16:37:00.332108Z"
    }
   },
   "source": [
    "import torch # 导入torch\n",
    "from transformers import GPT2Tokenizer # 导入GPT2分词器\n",
    "from transformers import GPT2LMHeadModel # 导入GPT2语言模型\n",
    "\n",
    "model_name = \"gpt2\"  # 也可以选择其他模型，如\"gpt2-medium\"、\"gpt2-large\"等\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name) # 加载分词器\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # 判断是否有可用GPU\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device) # 将模型加载到设备上（CPU或GPU）\n",
    "vocab = tokenizer.get_vocab() # 获取词汇表\n",
    "\n",
    "# 示例RLHF数据集\n",
    "data = [\n",
    "    {\n",
    "        \"User\": \"What is the capital of France?\",\n",
    "        # \"AI\": \"The capital of France is Paris.\",\n",
    "        \"AI\": \"Paris.\",\n",
    "        \"score\": 5\n",
    "    },\n",
    "    {\n",
    "        \"User\": \"What is the capital of France?\",\n",
    "        \"AI\": \"Rome.\",\n",
    "        \"score\": 1\n",
    "    },\n",
    "    {\n",
    "        \"User\": \"How to cook pasta?\",\n",
    "        # \"AI\": \"To cook pasta, first boil water and then add pasta.\",\n",
    "        \"AI\": \"first boil water.\",\n",
    "        \"score\": 4\n",
    "    },\n",
    "    {\n",
    "        \"User\": \"How to cook pasta?\",\n",
    "        # \"AI\": \"First, turn on the microwave and put the pasta inside.\",\n",
    "        \"AI\": \"microwave.\",\n",
    "        \"score\": 2\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:37:44.274197Z",
     "start_time": "2024-11-28T16:37:44.254186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset  # 导入Pytorch的Dataset\n",
    "class RLHFDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab):\n",
    "        self.tokenizer = tokenizer  # 分词器\n",
    "        self.vocab = vocab  # 词汇表\n",
    "        self.input_data, self.target_data, self.scores = self.process_data(data)\n",
    "        \n",
    "    def process_data(self, data):        \n",
    "        input_data, target_data, scores = [], [], []       \n",
    "        for conversation in data:\n",
    "            user_question = conversation[\"User\"]\n",
    "            model_answer = conversation[\"AI\"]\n",
    "            score = conversation[\"score\"]\n",
    "\n",
    "            input_tokens = self.tokenizer(f\"{user_question}\", return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
    "            input_tokens = input_tokens + [tokenizer.eos_token_id]\n",
    "            input_data.append(torch.tensor(input_tokens, dtype=torch.long))\n",
    "\n",
    "            target_tokens = self.tokenizer(model_answer, return_tensors=\"pt\")[\"input_ids\"].tolist()[0]\n",
    "            target_tokens = target_tokens + [tokenizer.eos_token_id]\n",
    "            target_data.append(torch.tensor(target_tokens, dtype=torch.long))\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        return input_data, target_data, scores\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx], self.scores[idx]\n",
    "\n",
    "rlhf_dataset = RLHFDataset(data, tokenizer, vocab) # 创建ChatDataset对象，传入文件、分词器和词汇表\n",
    "# 打印数据集中前2个数据示例\n",
    "for i in range(2):\n",
    "    input_example, target_example, _ = rlhf_dataset[i]\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(\"Input:\", tokenizer.decode(input_example))\n",
    "    print(\"Target:\", tokenizer.decode(target_example))"
   ],
   "id": "e6fd38c4b279f1d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Input: What is the capital of France?<|endoftext|>\n",
      "Target: Paris.<|endoftext|>\n",
      "Example 2:\n",
      "Input: What is the capital of France?<|endoftext|>\n",
      "Target: Rome.<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:38:28.182323Z",
     "start_time": "2024-11-28T16:38:28.126098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader # 导入Dataloader\n",
    "tokenizer.pad_token = '<pad>' # 为分词器添加pad token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "# 定义pad_sequence函数，用于将一批序列补齐到相同长度\n",
    "def pad_sequence(sequences, padding_value=0, length=None):\n",
    "    # 计算最大序列长度，如果length参数未提供，则使用输入序列中的最大长度\n",
    "    max_length = max(len(seq) for seq in sequences) if length is None else length    \n",
    "    # 创建一个具有适当形状的全零张量，用于存储补齐后的序列\n",
    "    result = torch.full((len(sequences), max_length), padding_value, dtype=torch.long)    \n",
    "    # 遍历序列，将每个序列的内容复制到结果张量中\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = len(seq)\n",
    "        result[i, :end] = seq[:end]\n",
    "    return result\n",
    "\n",
    "# 定义collate_fn函数，用于将一个批次的数据整理成适当的形状\n",
    "def collate_fn(batch):\n",
    "    # 从批次中分离源序列、目标序列和分数\n",
    "    sources, targets, scores = zip(*batch)    \n",
    "    # 计算批次中的最大序列长度\n",
    "    max_length = max(max(len(s) for s in sources), max(len(t) for t in targets))    \n",
    "    # 使用 pad_sequence 函数补齐源序列和目标序列\n",
    "    sources = pad_sequence(sources, padding_value=tokenizer.pad_token_id, length=max_length)\n",
    "    targets = pad_sequence(targets, padding_value=tokenizer.pad_token_id, length=max_length)\n",
    "    # 将分数转换为张量\n",
    "    scores = torch.tensor(scores, dtype=torch.float)\n",
    "    # 返回补齐后的源序列、目标序列和分数\n",
    "    return sources, targets, scores\n",
    "\n",
    "# 创建Dataloader\n",
    "batch_size = 2\n",
    "chat_dataloader = DataLoader(rlhf_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 检查Dataloader输出\n",
    "for input_batch, target_batch, score_batch in chat_dataloader:\n",
    "    print(\"Input batch tensor size:\", input_batch.size())\n",
    "    print(\"Target batch tensor size:\", target_batch.size())\n",
    "    print(\"Score batch tensor size:\", score_batch.size())\n",
    "    break"
   ],
   "id": "6a238e1692f7dded",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch tensor size: torch.Size([2, 8])\n",
      "Target batch tensor size: torch.Size([2, 8])\n",
      "Score batch tensor size: torch.Size([2])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:38:58.054167Z",
     "start_time": "2024-11-28T16:38:58.033156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reward_function(predictions, targets, scores):\n",
    "    correct = (predictions == targets).float() * scores.unsqueeze(1)\n",
    "    reward = correct.sum(dim=-1) / (targets != tokenizer.pad_token_id).sum(dim=-1).float()\n",
    "    return reward / scores.max()"
   ],
   "id": "5dd94aca9dc70e7a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:39:41.127267Z",
     "start_time": "2024-11-28T16:39:20.070477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# 训练过程\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_rewards = []\n",
    "    \n",
    "    for batch_idx, (input_batch, target_batch, score_batch) in enumerate(chat_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "        score_batch = score_batch.to(device)\n",
    "        \n",
    "        outputs = model(input_batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, predicted_tokens = torch.max(logits, dim=-1)\n",
    "        \n",
    "        # 计算奖励\n",
    "        rewards = reward_function(predicted_tokens, target_batch, score_batch)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), target_batch.view(-1))\n",
    "        \n",
    "        # 计算加权损失\n",
    "        weighted_loss = torch.sum(loss * (1 - rewards)) / rewards.numel()\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        weighted_loss.backward()\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_rewards.append(rewards.cpu().numpy())\n",
    "    \n",
    "    avg_reward = np.mean(np.concatenate(epoch_rewards))\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch: {epoch + 1:04d}, cost = {weighted_loss:.6f}, avg_reward = {avg_reward:.6f}')"
   ],
   "id": "41519875301e7080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020, cost = 0.458659, avg_reward = 0.466667\n",
      "Epoch: 0040, cost = 0.270218, avg_reward = 0.379167\n",
      "Epoch: 0060, cost = 0.606063, avg_reward = 0.237500\n",
      "Epoch: 0080, cost = 0.891567, avg_reward = 0.341667\n",
      "Epoch: 0100, cost = 0.335679, avg_reward = 0.454167\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:40:08.442310Z",
     "start_time": "2024-11-28T16:39:53.975081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_beam_search(model, input_str, max_len=50, beam_width=5):\n",
    "    model.eval()  # 将模型设置为评估模式（不计算梯度）\n",
    "    # 对输入字符串进行编码，并将其转换为 PyTorch 张量，然后将其移动到相应的设备上（例如 GPU）\n",
    "    input_tokens = tokenizer.encode(input_str, return_tensors=\"pt\").to(device)    \n",
    "    # 初始化候选序列列表，包含当前输入序列和其对数概率得分（我们从0开始）\n",
    "    candidates = [(input_tokens, 0.0)]    \n",
    "    # 禁用梯度计算，以加速预测过程\n",
    "    with torch.no_grad():\n",
    "        # 迭代生成最大长度的序列\n",
    "        for _ in range(max_len):\n",
    "            new_candidates = []            \n",
    "            # 对于每个候选序列\n",
    "            for candidate, candidate_score in candidates:\n",
    "                # 使用模型进行预测\n",
    "                outputs = model(candidate)\n",
    "                # 获取输出 logits\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                # 获取对数概率得分的 top-k 值（即 beam_width）及其对应的 token\n",
    "                scores, next_tokens = torch.topk(logits, beam_width, dim=-1)\n",
    "                final_results = []\n",
    "                # 遍历 top-k token 及其对应的得分\n",
    "                for score, next_token in zip(scores.squeeze(), next_tokens.squeeze()):\n",
    "                    # 在当前候选序列中添加新的 token\n",
    "                    new_candidate = torch.cat((candidate, next_token.unsqueeze(0).unsqueeze(0)), dim=-1)\n",
    "                    # 更新候选序列的得分\n",
    "                    new_score = candidate_score - score.item()                    \n",
    "                    # 如果新的 token 是结束符（eos_token），则将该候选序列添加到最终结果中\n",
    "                    if next_token.item() == tokenizer.eos_token_id:\n",
    "                        final_results.append((new_candidate, new_score))\n",
    "                    # 否则，将新的候选序列添加到新候选序列列表中\n",
    "                    else:\n",
    "                        new_candidates.append((new_candidate, new_score))            \n",
    "            # 从新候选序列列表中选择得分最高的 top-k 个序列\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1])[:beam_width]    \n",
    "    # 选择得分最高的候选序列\n",
    "    best_candidate, _ = sorted(candidates, key=lambda x: x[1])[0]    \n",
    "    # 将输出 token 转换回文本字符串\n",
    "    output_str = tokenizer.decode(best_candidate[0])    \n",
    "    # 移除输入字符串并修复空格问题\n",
    "    input_len = len(tokenizer.encode(input_str))\n",
    "    output_str = tokenizer.decode(best_candidate.squeeze()[input_len:])    \n",
    "    return output_str\n",
    "\n",
    "test_inputs = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How to cook pasta?\",\n",
    "    \"hi , what is your name?\"\n",
    "]\n",
    "\n",
    "for i, input_str in enumerate(test_inputs, start=1):\n",
    "    generated_text = generate_text_beam_search(model, input_str)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"User: {input_str}\")\n",
    "    print(f\"AI: {generated_text}\")\n",
    "    print()"
   ],
   "id": "687d1e287c311c5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n",
      "User: What is the capital of France?\n",
      "AI:  A.Romeoomea, aomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeomeome\n",
      "\n",
      "Test 2:\n",
      "User: How to cook pasta?\n",
      "AI:  The pasta water in water wateraveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveaveave\n",
      "\n",
      "Test 3:\n",
      "User: hi , what is your name?\n",
      "AI:  The one with all of this, the one with all this, the one with all this, the one with everything all this, the one with all this, the all of all this, all this, the, all this, the, the,\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e46d4acce0ee47cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
