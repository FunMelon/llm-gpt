{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T06:54:34.932404Z",
     "start_time": "2024-11-24T06:54:34.910939Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1.创建两个张量x1和x2\n",
    "x1 = torch.randn(2, 3, 4)  # 形状(batch_size, seq_len1, feature_dim)\n",
    "x2 = torch.randn(2, 5, 4)  # 形状(batch_size, seq_len2, feature_dim)\n",
    "\n",
    "# 2.计算原始权重\n",
    "raw_weights = torch.bmm(x1, x2.transpose(1, 2))  # 形状为(batch_size, seq_len1, seq_len2)\n",
    "\n",
    "# 2.5.对原始权重进行缩放\n",
    "d = x1.size(-1)  # feature_dim\n",
    "scaled_weights = raw_weights / (d ** 0.5)\n",
    "\n",
    "# 3.用softmax函数对原始权重进行归一化\n",
    "attn_weights = F.softmax(scaled_weights, dim=2)  # 形状为(batch_size, seq_len1, seq_len2)\n",
    "\n",
    "# 4.将注意力权重与x2相乘，得到加权和\n",
    "attn_output = torch.bmm(attn_weights, x2)  # 形状为(batch_size, seq_len1, feature_dim)\n",
    "\n",
    "print(\"x1:\", x1[0])\n",
    "print(\"x2:\", x2[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([[-0.1168,  1.2077,  1.6158, -0.1350],\n",
      "        [-0.2722,  1.2552,  0.0547,  1.9958],\n",
      "        [-0.2110,  0.8115,  0.3279, -0.0909]])\n",
      "x2: tensor([[-2.1248,  0.1422, -1.3766, -0.1223],\n",
      "        [-0.1991, -0.7151, -1.1571,  0.1581],\n",
      "        [-0.2669, -0.4177, -0.7891, -0.4896],\n",
      "        [-1.4461, -0.8789, -0.2778, -0.7230],\n",
      "        [-0.1207, -0.0939,  0.6572, -0.5005]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:08:59.119046Z",
     "start_time": "2024-11-24T07:08:59.103361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分析注意力分数\n",
    "# 点积越大，两个向量的相似度越高\n",
    "print(\"raw_weight:\", raw_weights[0])\n",
    "_, max_idx = torch.max(raw_weights[0], 1)\n",
    "print(\"max_idx:\", max_idx)\n",
    "print(\"第一批次相似的向量\")\n",
    "for i, idx in enumerate(max_idx,):\n",
    "    print(\"x1:\", x1[0][i], \"x2;\", x2[0][idx])\n",
    "    \n",
    "# 使用余弦相似度来查看向量的相似度\n",
    "cos_sim_matrix = F.cosine_similarity(x1[0].unsqueeze(1), x2[0].unsqueeze(0), dim=2)\n",
    "print(\"cos_sim_matrix:\", cos_sim_matrix)\n",
    "_, max_idx = torch.max(raw_weights[0], 1)\n",
    "print(\"max_idx:\", max_idx)"
   ],
   "id": "1fbaa170e1d8f19c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_weight: tensor([[-1.7878, -2.7313, -1.6822, -1.2437,  1.0302],\n",
      "        [ 0.4375, -0.5911, -1.4719, -2.1677, -1.0480],\n",
      "        [ 0.1234, -0.9321, -0.4969, -0.4335,  0.2103]])\n",
      "max_idx: tensor([4, 0, 4])\n",
      "第一批次相似的向量\n",
      "x1: tensor([-0.1168,  1.2077,  1.6158, -0.1350]) x2; tensor([-0.1207, -0.0939,  0.6572, -0.5005])\n",
      "x1: tensor([-0.2722,  1.2552,  0.0547,  1.9958]) x2; tensor([-2.1248,  0.1422, -1.3766, -0.1223])\n",
      "x1: tensor([-0.2110,  0.8115,  0.3279, -0.0909]) x2; tensor([-0.1207, -0.0939,  0.6572, -0.5005])\n",
      "cos_sim_matrix: tensor([[-0.3478, -0.9747, -0.7891, -0.3300,  0.6055],\n",
      "        [ 0.0726, -0.1800, -0.5890, -0.4906, -0.5255],\n",
      "        [ 0.0537, -0.7444, -0.5216, -0.2574,  0.2766]])\n",
      "max_idx: tensor([4, 0, 4])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:10:11.813680Z",
     "start_time": "2024-11-24T07:10:11.808132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分析归一化后的权重\n",
    "print(\"attn_weights:\", attn_weights[0])"
   ],
   "id": "710e3b1f99d507c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights: tensor([[0.1237, 0.0772, 0.1304, 0.1624, 0.5063],\n",
      "        [0.3662, 0.2190, 0.1410, 0.0996, 0.1743],\n",
      "        [0.2424, 0.1430, 0.1778, 0.1835, 0.2532]])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:14:08.056816Z",
     "start_time": "2024-11-24T07:14:08.051784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分析加权和\n",
    "print(\"attn_output:\", attn_output[0])"
   ],
   "id": "13a1f4f815784049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: tensor([[-0.6090, -0.2823, -0.0749, -0.4376],\n",
      "        [-1.0244, -0.2672, -0.7819, -0.2384],\n",
      "        [-0.8870, -0.3271, -0.5241, -0.3535]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9140db15bec5eb3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
