{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-24T12:59:46.407977Z",
     "start_time": "2024-11-24T12:59:46.393982Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# 一个形状为(batch_size, seq_len, feature_dim)的张量\n",
    "x = torch.randn(2, 3, 4)  # 形状为(batch_size, seq_len, feature_dim)\n",
    "# 定义头数和每个头的维度\n",
    "num_heads = 2\n",
    "head_dim = 2\n",
    "# feature_dim必须是num_heads * head_dim的倍数\n",
    "assert x.size(-1) == num_heads * head_dim\n",
    "# 定义线性层用于将x转换为Q, K, V向量\n",
    "linear_q = torch.nn.Linear(4, 4)\n",
    "linear_k = torch.nn.Linear(4, 4)\n",
    "linear_v = torch.nn.Linear(4, 4)\n",
    "# 将x转换为Q, K, V向量\n",
    "Q = linear_q(x) # 形状为(batch_size, seq_len, feature_dim)\n",
    "K = linear_k(x) # 形状为(batch_size, seq_len, feature_dim)\n",
    "V = linear_v(x) # 形状为(batch_size, seq_len, feature_dim)\n",
    "# 将Q, K, V向量分割为num_heads个头\n",
    "def split_heads(tensor, num_heads):\n",
    "    batch_size, seq_len, feature_dim = tensor.size()\n",
    "    head_dim = feature_dim // num_heads\n",
    "    output = tensor.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "    return output # 形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "Q = split_heads(Q, num_heads) # 形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "K = split_heads(K, num_heads) # 形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "V = split_heads(V, num_heads) # 形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "# 计算Q和K的点积，作为相似度分数，也就是自注意力原始权重\n",
    "raw_weights = torch.matmul(Q, K.transpose(-1, -2)) # 形状为(batch_size, num_heads, seq_len, seq_len)\n",
    "# 对自注意力原始权重进行缩放\n",
    "scaled_weights = raw_weights / (Q.size(-1) ** 0.5)\n",
    "# 对权重进行softmax操作\n",
    "attn_weights = F.softmax(scaled_weights, dim=-1) # 形状为(batch_size, num_heads, seq_len, seq_len)\n",
    "# 使用注意力权重对V进行加权平均\n",
    "attention_outputs = torch.matmul(attn_weights, V) # 形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "# 将多头注意力的输出重组为原始形状\n",
    "def combine_heads(tensor):\n",
    "    batch_size, num_heads, seq_len, head_dim = tensor.size()\n",
    "    output = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, num_heads * head_dim)\n",
    "    return output # 形状为(batch_size, seq_len, feature_dim)\n",
    "attn_outputs = combine_heads(attention_outputs) # 形状为(batch_size, seq_len, feature_dim)\n",
    "# 对多头注意力的输出进行线性变换\n",
    "linear = torch.nn.Linear(4, 4)\n",
    "attn_outputs = linear(attn_outputs) # 形状为(batch_size, seq_len, feature_dim)\n",
    "print(\"加权信息：\", attn_outputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权信息： tensor([[[ 0.0143, -0.3493, -0.5976,  0.2996],\n",
      "         [-0.1211, -0.4357, -0.6027,  0.3403],\n",
      "         [ 0.0272, -0.3429, -0.6007,  0.3015]],\n",
      "\n",
      "        [[-0.3424, -0.3070, -0.0536,  0.0419],\n",
      "         [-0.3738, -0.3230, -0.0479,  0.0585],\n",
      "         [-0.3385, -0.3043, -0.0551,  0.0508]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14034ea3bc086207"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
